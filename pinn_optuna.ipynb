{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 09:00:11.640314: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-11 09:00:11.768393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741658411.836478  115042 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741658411.854947  115042 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-11 09:00:11.968734: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/hung-tran-nam/Downloads/PINN_ODE/PINN_ODEs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0000 00:00:1741658418.003954  115042 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "[I 2025-03-11 09:00:18,218] A new study created in memory with name: no-name-c5cb0119-5b07-4f27-b2c2-de1b165307ae\n",
      "/home/hung-tran-nam/Downloads/PINN_ODE/PINN_ODEs/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "[I 2025-03-11 09:02:30,275] Trial 0 finished with value: 0.2011905014514923 and parameters: {'dropout_rate': 0.18231589384155433, 'num_neurons': 32, 'learning_rate': 0.004006674055296358}. Best is trial 0 with value: 0.2011905014514923.\n",
      "[I 2025-03-11 09:04:55,086] Trial 1 finished with value: 0.1527475118637085 and parameters: {'dropout_rate': 0.15090538812840676, 'num_neurons': 64, 'learning_rate': 0.0003042389046864262}. Best is trial 1 with value: 0.1527475118637085.\n",
      "[I 2025-03-11 09:07:04,033] Trial 2 finished with value: 0.5040322542190552 and parameters: {'dropout_rate': 0.47780119322730236, 'num_neurons': 16, 'learning_rate': 0.0035437714663291947}. Best is trial 1 with value: 0.1527475118637085.\n",
      "[I 2025-03-11 09:09:33,166] Trial 3 finished with value: 0.3361881971359253 and parameters: {'dropout_rate': 0.40607488332581015, 'num_neurons': 64, 'learning_rate': 0.001725603223914582}. Best is trial 1 with value: 0.1527475118637085.\n",
      "[I 2025-03-11 09:11:44,216] Trial 4 finished with value: 0.11996210366487503 and parameters: {'dropout_rate': 0.11837602740663444, 'num_neurons': 32, 'learning_rate': 0.0057810279247513825}. Best is trial 4 with value: 0.11996210366487503.\n",
      "[I 2025-03-11 09:12:47,335] Trial 5 finished with value: 0.5042743682861328 and parameters: {'dropout_rate': 0.42448923042466574, 'num_neurons': 8, 'learning_rate': 0.00380080674932354}. Best is trial 4 with value: 0.11996210366487503.\n",
      "[I 2025-03-11 09:13:49,312] Trial 6 finished with value: 0.08829515427350998 and parameters: {'dropout_rate': 0.25926690089514653, 'num_neurons': 128, 'learning_rate': 0.0012507537524592816}. Best is trial 6 with value: 0.08829515427350998.\n",
      "[I 2025-03-11 09:14:50,352] Trial 7 finished with value: 0.3618134558200836 and parameters: {'dropout_rate': 0.32126953287644294, 'num_neurons': 32, 'learning_rate': 0.0027807633785107387}. Best is trial 6 with value: 0.08829515427350998.\n",
      "[I 2025-03-11 09:15:54,269] Trial 8 finished with value: 0.21167920529842377 and parameters: {'dropout_rate': 0.2052023882702967, 'num_neurons': 16, 'learning_rate': 0.009979273043591433}. Best is trial 6 with value: 0.08829515427350998.\n",
      "[I 2025-03-11 09:17:03,719] Trial 9 finished with value: 0.5042824745178223 and parameters: {'dropout_rate': 0.3170278822577102, 'num_neurons': 32, 'learning_rate': 0.0002965592439188508}. Best is trial 6 with value: 0.08829515427350998.\n",
      "[I 2025-03-11 09:18:10,526] Trial 10 finished with value: 0.015083939768373966 and parameters: {'dropout_rate': 0.02522429462282219, 'num_neurons': 128, 'learning_rate': 0.0008450247731476447}. Best is trial 10 with value: 0.015083939768373966.\n",
      "[I 2025-03-11 09:19:25,617] Trial 11 finished with value: 0.01586400344967842 and parameters: {'dropout_rate': 0.010695273019292573, 'num_neurons': 128, 'learning_rate': 0.0006662187121474799}. Best is trial 10 with value: 0.015083939768373966.\n",
      "[I 2025-03-11 09:20:34,540] Trial 12 finished with value: 0.008076620288193226 and parameters: {'dropout_rate': 0.008996849292546233, 'num_neurons': 128, 'learning_rate': 0.0001076918924411647}. Best is trial 12 with value: 0.008076620288193226.\n",
      "[I 2025-03-11 09:21:46,745] Trial 13 finished with value: 0.003120910143479705 and parameters: {'dropout_rate': 0.0002037984254264491, 'num_neurons': 128, 'learning_rate': 0.00010385274525099984}. Best is trial 13 with value: 0.003120910143479705.\n",
      "[I 2025-03-11 09:22:57,553] Trial 14 finished with value: 0.06606172770261765 and parameters: {'dropout_rate': 0.08786643090137192, 'num_neurons': 128, 'learning_rate': 0.00011185829478678052}. Best is trial 13 with value: 0.003120910143479705.\n",
      "[I 2025-03-11 09:24:10,968] Trial 15 finished with value: 0.041941650211811066 and parameters: {'dropout_rate': 0.07347633740631625, 'num_neurons': 128, 'learning_rate': 0.00010788419514051535}. Best is trial 13 with value: 0.003120910143479705.\n",
      "[I 2025-03-11 09:25:18,551] Trial 16 finished with value: 0.23992769420146942 and parameters: {'dropout_rate': 0.009118103059751526, 'num_neurons': 8, 'learning_rate': 0.00020174334191426645}. Best is trial 13 with value: 0.003120910143479705.\n",
      "[I 2025-03-11 09:26:33,315] Trial 17 finished with value: 0.027217354625463486 and parameters: {'dropout_rate': 0.06079937074011271, 'num_neurons': 128, 'learning_rate': 0.0004774742892952094}. Best is trial 13 with value: 0.003120910143479705.\n",
      "[I 2025-03-11 09:27:43,112] Trial 18 finished with value: 0.05363500118255615 and parameters: {'dropout_rate': 0.13217515395636756, 'num_neurons': 128, 'learning_rate': 0.00017895582801672673}. Best is trial 13 with value: 0.003120910143479705.\n",
      "[I 2025-03-11 09:28:54,061] Trial 19 finished with value: 0.1426558494567871 and parameters: {'dropout_rate': 0.24171730756399337, 'num_neurons': 128, 'learning_rate': 0.00018359053023862213}. Best is trial 13 with value: 0.003120910143479705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'dropout_rate': 0.0002037984254264491, 'num_neurons': 128, 'learning_rate': 0.00010385274525099984}\n",
      "Epoch 0/1000, PINN Loss: 0.5060518383979797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115042/21645127.py:109: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  frames.append(imageio.imread(frame_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, PINN Loss: 0.5051500797271729\n",
      "Epoch 2/1000, PINN Loss: 0.5055239796638489\n",
      "Epoch 3/1000, PINN Loss: 0.504965603351593\n",
      "Epoch 4/1000, PINN Loss: 0.505274772644043\n",
      "Epoch 5/1000, PINN Loss: 0.5044108629226685\n",
      "Epoch 6/1000, PINN Loss: 0.5052171349525452\n",
      "Epoch 7/1000, PINN Loss: 0.5047990679740906\n",
      "Epoch 8/1000, PINN Loss: 0.504916250705719\n",
      "Epoch 9/1000, PINN Loss: 0.5050949454307556\n",
      "Epoch 10/1000, PINN Loss: 0.5044780373573303\n",
      "Epoch 11/1000, PINN Loss: 0.5048038959503174\n",
      "Epoch 12/1000, PINN Loss: 0.5046060085296631\n",
      "Epoch 13/1000, PINN Loss: 0.5047838091850281\n",
      "Epoch 14/1000, PINN Loss: 0.5047279000282288\n",
      "Epoch 15/1000, PINN Loss: 0.5057113170623779\n",
      "Epoch 16/1000, PINN Loss: 0.504765510559082\n",
      "Epoch 17/1000, PINN Loss: 0.5051983594894409\n",
      "Epoch 18/1000, PINN Loss: 0.504909336566925\n",
      "Epoch 19/1000, PINN Loss: 0.5050274133682251\n",
      "Epoch 20/1000, PINN Loss: 0.504604697227478\n",
      "Epoch 21/1000, PINN Loss: 0.5045807957649231\n",
      "Epoch 22/1000, PINN Loss: 0.5054275393486023\n",
      "Epoch 23/1000, PINN Loss: 0.505020022392273\n",
      "Epoch 24/1000, PINN Loss: 0.5049203038215637\n",
      "Epoch 25/1000, PINN Loss: 0.5043729543685913\n",
      "Epoch 26/1000, PINN Loss: 0.5049005150794983\n",
      "Epoch 27/1000, PINN Loss: 0.504572331905365\n",
      "Epoch 28/1000, PINN Loss: 0.5048030018806458\n",
      "Epoch 29/1000, PINN Loss: 0.5058253407478333\n",
      "Epoch 30/1000, PINN Loss: 0.5047361254692078\n",
      "Epoch 31/1000, PINN Loss: 0.5049943923950195\n",
      "Epoch 32/1000, PINN Loss: 0.5044755339622498\n",
      "Epoch 33/1000, PINN Loss: 0.5047224164009094\n",
      "Epoch 34/1000, PINN Loss: 0.5049514770507812\n",
      "Epoch 35/1000, PINN Loss: 0.5048848390579224\n",
      "Epoch 36/1000, PINN Loss: 0.5048812031745911\n",
      "Epoch 37/1000, PINN Loss: 0.504633903503418\n",
      "Epoch 38/1000, PINN Loss: 0.504461407661438\n",
      "Epoch 39/1000, PINN Loss: 0.5043631196022034\n",
      "Epoch 40/1000, PINN Loss: 0.5045009851455688\n",
      "Epoch 41/1000, PINN Loss: 0.5045334100723267\n",
      "Epoch 42/1000, PINN Loss: 0.5044399499893188\n",
      "Epoch 43/1000, PINN Loss: 0.5042785406112671\n",
      "Epoch 44/1000, PINN Loss: 0.5047612190246582\n",
      "Epoch 45/1000, PINN Loss: 0.5040376782417297\n",
      "Epoch 46/1000, PINN Loss: 0.5052754282951355\n",
      "Epoch 47/1000, PINN Loss: 0.5047180652618408\n",
      "Epoch 48/1000, PINN Loss: 0.5040152668952942\n",
      "Epoch 49/1000, PINN Loss: 0.505362868309021\n",
      "Epoch 50/1000, PINN Loss: 0.5044121742248535\n",
      "Epoch 51/1000, PINN Loss: 0.5046404004096985\n",
      "Epoch 52/1000, PINN Loss: 0.5040934681892395\n",
      "Epoch 53/1000, PINN Loss: 0.5041911602020264\n",
      "Epoch 54/1000, PINN Loss: 0.504539430141449\n",
      "Epoch 55/1000, PINN Loss: 0.5044059753417969\n",
      "Epoch 56/1000, PINN Loss: 0.5047695636749268\n",
      "Epoch 57/1000, PINN Loss: 0.5042365789413452\n",
      "Epoch 58/1000, PINN Loss: 0.5042206645011902\n",
      "Epoch 59/1000, PINN Loss: 0.5046382546424866\n",
      "Epoch 60/1000, PINN Loss: 0.5035985708236694\n",
      "Epoch 61/1000, PINN Loss: 0.5045329928398132\n",
      "Epoch 62/1000, PINN Loss: 0.504168689250946\n",
      "Epoch 63/1000, PINN Loss: 0.5047239065170288\n",
      "Epoch 64/1000, PINN Loss: 0.5042058825492859\n",
      "Epoch 65/1000, PINN Loss: 0.5038889050483704\n",
      "Epoch 66/1000, PINN Loss: 0.5050472617149353\n",
      "Epoch 67/1000, PINN Loss: 0.5038121938705444\n",
      "Epoch 68/1000, PINN Loss: 0.5043359994888306\n",
      "Epoch 69/1000, PINN Loss: 0.504276692867279\n",
      "Epoch 70/1000, PINN Loss: 0.5037785172462463\n",
      "Epoch 71/1000, PINN Loss: 0.504276692867279\n",
      "Epoch 72/1000, PINN Loss: 0.5035427808761597\n",
      "Epoch 73/1000, PINN Loss: 0.504136323928833\n",
      "Epoch 74/1000, PINN Loss: 0.5032880306243896\n",
      "Epoch 75/1000, PINN Loss: 0.5040175914764404\n",
      "Epoch 76/1000, PINN Loss: 0.503421425819397\n",
      "Epoch 77/1000, PINN Loss: 0.503213107585907\n",
      "Epoch 78/1000, PINN Loss: 0.5039043426513672\n",
      "Epoch 79/1000, PINN Loss: 0.5039595365524292\n",
      "Epoch 80/1000, PINN Loss: 0.5034515261650085\n",
      "Epoch 81/1000, PINN Loss: 0.5034441947937012\n",
      "Epoch 82/1000, PINN Loss: 0.5029215812683105\n",
      "Epoch 83/1000, PINN Loss: 0.5038195252418518\n",
      "Epoch 84/1000, PINN Loss: 0.5035510659217834\n",
      "Epoch 85/1000, PINN Loss: 0.5036756992340088\n",
      "Epoch 86/1000, PINN Loss: 0.5036867260932922\n",
      "Epoch 87/1000, PINN Loss: 0.5028172731399536\n",
      "Epoch 88/1000, PINN Loss: 0.5034511685371399\n",
      "Epoch 89/1000, PINN Loss: 0.5027145147323608\n",
      "Epoch 90/1000, PINN Loss: 0.503494918346405\n",
      "Epoch 91/1000, PINN Loss: 0.5033000111579895\n",
      "Epoch 92/1000, PINN Loss: 0.5033427476882935\n",
      "Epoch 93/1000, PINN Loss: 0.5031052827835083\n",
      "Epoch 94/1000, PINN Loss: 0.5030142664909363\n",
      "Epoch 95/1000, PINN Loss: 0.5029761791229248\n",
      "Epoch 96/1000, PINN Loss: 0.5032835006713867\n",
      "Epoch 97/1000, PINN Loss: 0.5034581422805786\n",
      "Epoch 98/1000, PINN Loss: 0.5031165480613708\n",
      "Epoch 99/1000, PINN Loss: 0.5030037760734558\n",
      "Epoch 100/1000, PINN Loss: 0.5030633807182312\n",
      "Epoch 101/1000, PINN Loss: 0.502733588218689\n",
      "Epoch 102/1000, PINN Loss: 0.5029239654541016\n",
      "Epoch 103/1000, PINN Loss: 0.5028772354125977\n",
      "Epoch 104/1000, PINN Loss: 0.5029956698417664\n",
      "Epoch 105/1000, PINN Loss: 0.5018581748008728\n",
      "Epoch 106/1000, PINN Loss: 0.503074586391449\n",
      "Epoch 107/1000, PINN Loss: 0.5030701756477356\n",
      "Epoch 108/1000, PINN Loss: 0.5027289986610413\n",
      "Epoch 109/1000, PINN Loss: 0.5027430653572083\n",
      "Epoch 110/1000, PINN Loss: 0.5025624632835388\n",
      "Epoch 111/1000, PINN Loss: 0.5028889179229736\n",
      "Epoch 112/1000, PINN Loss: 0.5025352239608765\n",
      "Epoch 113/1000, PINN Loss: 0.5019721388816833\n",
      "Epoch 114/1000, PINN Loss: 0.5028579235076904\n",
      "Epoch 115/1000, PINN Loss: 0.5024760961532593\n",
      "Epoch 116/1000, PINN Loss: 0.5023071765899658\n",
      "Epoch 117/1000, PINN Loss: 0.5024377107620239\n",
      "Epoch 118/1000, PINN Loss: 0.5025578141212463\n",
      "Epoch 119/1000, PINN Loss: 0.5025338530540466\n",
      "Epoch 120/1000, PINN Loss: 0.5016095042228699\n",
      "Epoch 121/1000, PINN Loss: 0.5022997260093689\n",
      "Epoch 122/1000, PINN Loss: 0.5021166801452637\n",
      "Epoch 123/1000, PINN Loss: 0.5017789602279663\n",
      "Epoch 124/1000, PINN Loss: 0.5021086931228638\n",
      "Epoch 125/1000, PINN Loss: 0.5019822716712952\n",
      "Epoch 126/1000, PINN Loss: 0.5016164183616638\n",
      "Epoch 127/1000, PINN Loss: 0.5016900300979614\n",
      "Epoch 128/1000, PINN Loss: 0.5019230842590332\n",
      "Epoch 129/1000, PINN Loss: 0.5014323592185974\n",
      "Epoch 130/1000, PINN Loss: 0.5015387535095215\n",
      "Epoch 131/1000, PINN Loss: 0.5014983415603638\n",
      "Epoch 132/1000, PINN Loss: 0.5017200112342834\n",
      "Epoch 133/1000, PINN Loss: 0.5013378262519836\n",
      "Epoch 134/1000, PINN Loss: 0.5018828511238098\n",
      "Epoch 135/1000, PINN Loss: 0.5014143586158752\n",
      "Epoch 136/1000, PINN Loss: 0.5022521615028381\n",
      "Epoch 137/1000, PINN Loss: 0.5020982623100281\n",
      "Epoch 138/1000, PINN Loss: 0.500996470451355\n",
      "Epoch 139/1000, PINN Loss: 0.5011579990386963\n",
      "Epoch 140/1000, PINN Loss: 0.5008029341697693\n",
      "Epoch 141/1000, PINN Loss: 0.5008512139320374\n",
      "Epoch 142/1000, PINN Loss: 0.5021108984947205\n",
      "Epoch 143/1000, PINN Loss: 0.5007070302963257\n",
      "Epoch 144/1000, PINN Loss: 0.5008656978607178\n",
      "Epoch 145/1000, PINN Loss: 0.5006862878799438\n",
      "Epoch 146/1000, PINN Loss: 0.5001660585403442\n",
      "Epoch 147/1000, PINN Loss: 0.500249981880188\n",
      "Epoch 148/1000, PINN Loss: 0.5002103447914124\n",
      "Epoch 149/1000, PINN Loss: 0.5010644793510437\n",
      "Epoch 150/1000, PINN Loss: 0.49973300099372864\n",
      "Epoch 151/1000, PINN Loss: 0.4998719394207001\n",
      "Epoch 152/1000, PINN Loss: 0.5000500679016113\n",
      "Epoch 153/1000, PINN Loss: 0.49984681606292725\n",
      "Epoch 154/1000, PINN Loss: 0.5000954866409302\n",
      "Epoch 155/1000, PINN Loss: 0.49932610988616943\n",
      "Epoch 156/1000, PINN Loss: 0.5002570152282715\n",
      "Epoch 157/1000, PINN Loss: 0.499002069234848\n",
      "Epoch 158/1000, PINN Loss: 0.5015609860420227\n",
      "Epoch 159/1000, PINN Loss: 0.4997807741165161\n",
      "Epoch 160/1000, PINN Loss: 0.49925342202186584\n",
      "Epoch 161/1000, PINN Loss: 0.4994127154350281\n",
      "Epoch 162/1000, PINN Loss: 0.49880027770996094\n",
      "Epoch 163/1000, PINN Loss: 0.4991941452026367\n",
      "Epoch 164/1000, PINN Loss: 0.49825751781463623\n",
      "Epoch 165/1000, PINN Loss: 0.49875009059906006\n",
      "Epoch 166/1000, PINN Loss: 0.4978526532649994\n",
      "Epoch 167/1000, PINN Loss: 0.4980623126029968\n",
      "Epoch 168/1000, PINN Loss: 0.4978010356426239\n",
      "Epoch 169/1000, PINN Loss: 0.49777016043663025\n",
      "Epoch 170/1000, PINN Loss: 0.4983729124069214\n",
      "Epoch 171/1000, PINN Loss: 0.4983890652656555\n",
      "Epoch 172/1000, PINN Loss: 0.49808087944984436\n",
      "Epoch 173/1000, PINN Loss: 0.4980253577232361\n",
      "Epoch 174/1000, PINN Loss: 0.4970770478248596\n",
      "Epoch 175/1000, PINN Loss: 0.4964365065097809\n",
      "Epoch 176/1000, PINN Loss: 0.49634501338005066\n",
      "Epoch 177/1000, PINN Loss: 0.4958367943763733\n",
      "Epoch 178/1000, PINN Loss: 0.4963494539260864\n",
      "Epoch 179/1000, PINN Loss: 0.49654796719551086\n",
      "Epoch 180/1000, PINN Loss: 0.4964127242565155\n",
      "Epoch 181/1000, PINN Loss: 0.49579811096191406\n",
      "Epoch 182/1000, PINN Loss: 0.4954024851322174\n",
      "Epoch 183/1000, PINN Loss: 0.4953663647174835\n",
      "Epoch 184/1000, PINN Loss: 0.4956333637237549\n",
      "Epoch 185/1000, PINN Loss: 0.49513372778892517\n",
      "Epoch 186/1000, PINN Loss: 0.494613379240036\n",
      "Epoch 187/1000, PINN Loss: 0.4947246015071869\n",
      "Epoch 188/1000, PINN Loss: 0.4950171113014221\n",
      "Epoch 189/1000, PINN Loss: 0.49425914883613586\n",
      "Epoch 190/1000, PINN Loss: 0.4941280484199524\n",
      "Epoch 191/1000, PINN Loss: 0.4932345747947693\n",
      "Epoch 192/1000, PINN Loss: 0.49398645758628845\n",
      "Epoch 193/1000, PINN Loss: 0.49327871203422546\n",
      "Epoch 194/1000, PINN Loss: 0.49429818987846375\n",
      "Epoch 195/1000, PINN Loss: 0.49383294582366943\n",
      "Epoch 196/1000, PINN Loss: 0.49200406670570374\n",
      "Epoch 197/1000, PINN Loss: 0.4916209578514099\n",
      "Epoch 198/1000, PINN Loss: 0.49231821298599243\n",
      "Epoch 199/1000, PINN Loss: 0.4926894009113312\n",
      "Epoch 200/1000, PINN Loss: 0.491946280002594\n",
      "Epoch 201/1000, PINN Loss: 0.4914056658744812\n",
      "Epoch 202/1000, PINN Loss: 0.4916331470012665\n",
      "Epoch 203/1000, PINN Loss: 0.4908209443092346\n",
      "Epoch 204/1000, PINN Loss: 0.49052926898002625\n",
      "Epoch 205/1000, PINN Loss: 0.49058327078819275\n",
      "Epoch 206/1000, PINN Loss: 0.4903942346572876\n",
      "Epoch 207/1000, PINN Loss: 0.4896698296070099\n",
      "Epoch 208/1000, PINN Loss: 0.4899521470069885\n",
      "Epoch 209/1000, PINN Loss: 0.4891127943992615\n",
      "Epoch 210/1000, PINN Loss: 0.4877672493457794\n",
      "Epoch 211/1000, PINN Loss: 0.4885346293449402\n",
      "Epoch 212/1000, PINN Loss: 0.4865224361419678\n",
      "Epoch 213/1000, PINN Loss: 0.4878405034542084\n",
      "Epoch 214/1000, PINN Loss: 0.48763731122016907\n",
      "Epoch 215/1000, PINN Loss: 0.4889509379863739\n",
      "Epoch 216/1000, PINN Loss: 0.486433207988739\n",
      "Epoch 217/1000, PINN Loss: 0.4863015413284302\n",
      "Epoch 218/1000, PINN Loss: 0.48602935671806335\n",
      "Epoch 219/1000, PINN Loss: 0.4861733615398407\n",
      "Epoch 220/1000, PINN Loss: 0.48545488715171814\n",
      "Epoch 221/1000, PINN Loss: 0.4847719967365265\n",
      "Epoch 222/1000, PINN Loss: 0.48523834347724915\n",
      "Epoch 223/1000, PINN Loss: 0.4835140109062195\n",
      "Epoch 224/1000, PINN Loss: 0.48245009779930115\n",
      "Epoch 225/1000, PINN Loss: 0.4830992519855499\n",
      "Epoch 226/1000, PINN Loss: 0.4830978810787201\n",
      "Epoch 227/1000, PINN Loss: 0.4815835654735565\n",
      "Epoch 228/1000, PINN Loss: 0.48197564482688904\n",
      "Epoch 229/1000, PINN Loss: 0.4825243353843689\n",
      "Epoch 230/1000, PINN Loss: 0.48193132877349854\n",
      "Epoch 231/1000, PINN Loss: 0.48047614097595215\n",
      "Epoch 232/1000, PINN Loss: 0.4802931547164917\n",
      "Epoch 233/1000, PINN Loss: 0.4803391098976135\n",
      "Epoch 234/1000, PINN Loss: 0.4791976809501648\n",
      "Epoch 235/1000, PINN Loss: 0.47853949666023254\n",
      "Epoch 236/1000, PINN Loss: 0.4780089855194092\n",
      "Epoch 237/1000, PINN Loss: 0.4777904450893402\n",
      "Epoch 238/1000, PINN Loss: 0.4763241708278656\n",
      "Epoch 239/1000, PINN Loss: 0.4797302782535553\n",
      "Epoch 240/1000, PINN Loss: 0.4757384955883026\n",
      "Epoch 241/1000, PINN Loss: 0.47549790143966675\n",
      "Epoch 242/1000, PINN Loss: 0.47487780451774597\n",
      "Epoch 243/1000, PINN Loss: 0.47389477491378784\n",
      "Epoch 244/1000, PINN Loss: 0.473623663187027\n",
      "Epoch 245/1000, PINN Loss: 0.473810076713562\n",
      "Epoch 246/1000, PINN Loss: 0.47234785556793213\n",
      "Epoch 247/1000, PINN Loss: 0.4712409973144531\n",
      "Epoch 248/1000, PINN Loss: 0.4705767333507538\n",
      "Epoch 249/1000, PINN Loss: 0.47078439593315125\n",
      "Epoch 250/1000, PINN Loss: 0.469806045293808\n",
      "Epoch 251/1000, PINN Loss: 0.4686606526374817\n",
      "Epoch 252/1000, PINN Loss: 0.46751391887664795\n",
      "Epoch 253/1000, PINN Loss: 0.4672130048274994\n",
      "Epoch 254/1000, PINN Loss: 0.4668048620223999\n",
      "Epoch 255/1000, PINN Loss: 0.46503955125808716\n",
      "Epoch 256/1000, PINN Loss: 0.4652700126171112\n",
      "Epoch 257/1000, PINN Loss: 0.4643038213253021\n",
      "Epoch 258/1000, PINN Loss: 0.46281948685646057\n",
      "Epoch 259/1000, PINN Loss: 0.4626600742340088\n",
      "Epoch 260/1000, PINN Loss: 0.46191567182540894\n",
      "Epoch 261/1000, PINN Loss: 0.4617927074432373\n",
      "Epoch 262/1000, PINN Loss: 0.4599713981151581\n",
      "Epoch 263/1000, PINN Loss: 0.4593350887298584\n",
      "Epoch 264/1000, PINN Loss: 0.4579840898513794\n",
      "Epoch 265/1000, PINN Loss: 0.45658066868782043\n",
      "Epoch 266/1000, PINN Loss: 0.45661094784736633\n",
      "Epoch 267/1000, PINN Loss: 0.45571169257164\n",
      "Epoch 268/1000, PINN Loss: 0.4550214111804962\n",
      "Epoch 269/1000, PINN Loss: 0.4534703493118286\n",
      "Epoch 270/1000, PINN Loss: 0.452715665102005\n",
      "Epoch 271/1000, PINN Loss: 0.45161664485931396\n",
      "Epoch 272/1000, PINN Loss: 0.4511803686618805\n",
      "Epoch 273/1000, PINN Loss: 0.4486426115036011\n",
      "Epoch 274/1000, PINN Loss: 0.4480653703212738\n",
      "Epoch 275/1000, PINN Loss: 0.44748279452323914\n",
      "Epoch 276/1000, PINN Loss: 0.4518292546272278\n",
      "Epoch 277/1000, PINN Loss: 0.4453005790710449\n",
      "Epoch 278/1000, PINN Loss: 0.44439664483070374\n",
      "Epoch 279/1000, PINN Loss: 0.44281715154647827\n",
      "Epoch 280/1000, PINN Loss: 0.44173529744148254\n",
      "Epoch 281/1000, PINN Loss: 0.4395565092563629\n",
      "Epoch 282/1000, PINN Loss: 0.43933263421058655\n",
      "Epoch 283/1000, PINN Loss: 0.43817776441574097\n",
      "Epoch 284/1000, PINN Loss: 0.43537718057632446\n",
      "Epoch 285/1000, PINN Loss: 0.4353427290916443\n",
      "Epoch 286/1000, PINN Loss: 0.4337727129459381\n",
      "Epoch 287/1000, PINN Loss: 0.43262359499931335\n",
      "Epoch 288/1000, PINN Loss: 0.4318300485610962\n",
      "Epoch 289/1000, PINN Loss: 0.43016424775123596\n",
      "Epoch 290/1000, PINN Loss: 0.4288107454776764\n",
      "Epoch 291/1000, PINN Loss: 0.4313352406024933\n",
      "Epoch 292/1000, PINN Loss: 0.4264722466468811\n",
      "Epoch 293/1000, PINN Loss: 0.4246368110179901\n",
      "Epoch 294/1000, PINN Loss: 0.4220786988735199\n",
      "Epoch 295/1000, PINN Loss: 0.4207971394062042\n",
      "Epoch 296/1000, PINN Loss: 0.41942185163497925\n",
      "Epoch 297/1000, PINN Loss: 0.4229872226715088\n",
      "Epoch 298/1000, PINN Loss: 0.4166450500488281\n",
      "Epoch 299/1000, PINN Loss: 0.4182772934436798\n",
      "Epoch 300/1000, PINN Loss: 0.41323432326316833\n",
      "Epoch 301/1000, PINN Loss: 0.4110349416732788\n",
      "Epoch 302/1000, PINN Loss: 0.40989458560943604\n",
      "Epoch 303/1000, PINN Loss: 0.40830323100090027\n",
      "Epoch 304/1000, PINN Loss: 0.4045259654521942\n",
      "Epoch 305/1000, PINN Loss: 0.403289794921875\n",
      "Epoch 306/1000, PINN Loss: 0.402819961309433\n",
      "Epoch 307/1000, PINN Loss: 0.4015498757362366\n",
      "Epoch 308/1000, PINN Loss: 0.398908406496048\n",
      "Epoch 309/1000, PINN Loss: 0.3956989049911499\n",
      "Epoch 310/1000, PINN Loss: 0.3952413499355316\n",
      "Epoch 311/1000, PINN Loss: 0.3923780918121338\n",
      "Epoch 312/1000, PINN Loss: 0.3913137912750244\n",
      "Epoch 313/1000, PINN Loss: 0.3894205689430237\n",
      "Epoch 314/1000, PINN Loss: 0.3874806761741638\n",
      "Epoch 315/1000, PINN Loss: 0.3884955048561096\n",
      "Epoch 316/1000, PINN Loss: 0.3822929263114929\n",
      "Epoch 317/1000, PINN Loss: 0.3813745379447937\n",
      "Epoch 318/1000, PINN Loss: 0.3787590563297272\n",
      "Epoch 319/1000, PINN Loss: 0.37473800778388977\n",
      "Epoch 320/1000, PINN Loss: 0.37420469522476196\n",
      "Epoch 321/1000, PINN Loss: 0.3704409897327423\n",
      "Epoch 322/1000, PINN Loss: 0.36954495310783386\n",
      "Epoch 323/1000, PINN Loss: 0.36734166741371155\n",
      "Epoch 324/1000, PINN Loss: 0.36475318670272827\n",
      "Epoch 325/1000, PINN Loss: 0.36220863461494446\n",
      "Epoch 326/1000, PINN Loss: 0.36496835947036743\n",
      "Epoch 327/1000, PINN Loss: 0.35689833760261536\n",
      "Epoch 328/1000, PINN Loss: 0.35563522577285767\n",
      "Epoch 329/1000, PINN Loss: 0.3527667224407196\n",
      "Epoch 330/1000, PINN Loss: 0.35178858041763306\n",
      "Epoch 331/1000, PINN Loss: 0.34751200675964355\n",
      "Epoch 332/1000, PINN Loss: 0.3442891240119934\n",
      "Epoch 333/1000, PINN Loss: 0.34249234199523926\n",
      "Epoch 334/1000, PINN Loss: 0.3388313353061676\n",
      "Epoch 335/1000, PINN Loss: 0.33615124225616455\n",
      "Epoch 336/1000, PINN Loss: 0.3355291485786438\n",
      "Epoch 337/1000, PINN Loss: 0.3310164213180542\n",
      "Epoch 338/1000, PINN Loss: 0.3285321593284607\n",
      "Epoch 339/1000, PINN Loss: 0.3257581293582916\n",
      "Epoch 340/1000, PINN Loss: 0.32262998819351196\n",
      "Epoch 341/1000, PINN Loss: 0.31878823041915894\n",
      "Epoch 342/1000, PINN Loss: 0.31622329354286194\n",
      "Epoch 343/1000, PINN Loss: 0.31398069858551025\n",
      "Epoch 344/1000, PINN Loss: 0.3114096224308014\n",
      "Epoch 345/1000, PINN Loss: 0.308156281709671\n",
      "Epoch 346/1000, PINN Loss: 0.30565303564071655\n",
      "Epoch 347/1000, PINN Loss: 0.30233627557754517\n",
      "Epoch 348/1000, PINN Loss: 0.2987701892852783\n",
      "Epoch 349/1000, PINN Loss: 0.2955002784729004\n",
      "Epoch 350/1000, PINN Loss: 0.2928036153316498\n",
      "Epoch 351/1000, PINN Loss: 0.28918707370758057\n",
      "Epoch 352/1000, PINN Loss: 0.2861354649066925\n",
      "Epoch 353/1000, PINN Loss: 0.2837883532047272\n",
      "Epoch 354/1000, PINN Loss: 0.2796246409416199\n",
      "Epoch 355/1000, PINN Loss: 0.276326984167099\n",
      "Epoch 356/1000, PINN Loss: 0.27533724904060364\n",
      "Epoch 357/1000, PINN Loss: 0.27080127596855164\n",
      "Epoch 358/1000, PINN Loss: 0.2673822045326233\n",
      "Epoch 359/1000, PINN Loss: 0.26294007897377014\n",
      "Epoch 360/1000, PINN Loss: 0.25929418206214905\n",
      "Epoch 361/1000, PINN Loss: 0.2566567659378052\n",
      "Epoch 362/1000, PINN Loss: 0.2546277940273285\n",
      "Epoch 363/1000, PINN Loss: 0.25014767050743103\n",
      "Epoch 364/1000, PINN Loss: 0.24607336521148682\n",
      "Epoch 365/1000, PINN Loss: 0.24250735342502594\n",
      "Epoch 366/1000, PINN Loss: 0.23966330289840698\n",
      "Epoch 367/1000, PINN Loss: 0.23533238470554352\n",
      "Epoch 368/1000, PINN Loss: 0.23269161581993103\n",
      "Epoch 369/1000, PINN Loss: 0.2277686893939972\n",
      "Epoch 370/1000, PINN Loss: 0.22542423009872437\n",
      "Epoch 371/1000, PINN Loss: 0.22191011905670166\n",
      "Epoch 372/1000, PINN Loss: 0.2192308008670807\n",
      "Epoch 373/1000, PINN Loss: 0.2140836864709854\n",
      "Epoch 374/1000, PINN Loss: 0.2106797993183136\n",
      "Epoch 375/1000, PINN Loss: 0.20732633769512177\n",
      "Epoch 376/1000, PINN Loss: 0.20424652099609375\n",
      "Epoch 377/1000, PINN Loss: 0.19954249262809753\n",
      "Epoch 378/1000, PINN Loss: 0.19756220281124115\n",
      "Epoch 379/1000, PINN Loss: 0.19280081987380981\n",
      "Epoch 380/1000, PINN Loss: 0.19303597509860992\n",
      "Epoch 381/1000, PINN Loss: 0.18645687401294708\n",
      "Epoch 382/1000, PINN Loss: 0.18330256640911102\n",
      "Epoch 383/1000, PINN Loss: 0.17909061908721924\n",
      "Epoch 384/1000, PINN Loss: 0.1759999841451645\n",
      "Epoch 385/1000, PINN Loss: 0.17251721024513245\n",
      "Epoch 386/1000, PINN Loss: 0.1686106026172638\n",
      "Epoch 387/1000, PINN Loss: 0.16495126485824585\n",
      "Epoch 388/1000, PINN Loss: 0.160535991191864\n",
      "Epoch 389/1000, PINN Loss: 0.15812808275222778\n",
      "Epoch 390/1000, PINN Loss: 0.1537630558013916\n",
      "Epoch 391/1000, PINN Loss: 0.1509925276041031\n",
      "Epoch 392/1000, PINN Loss: 0.147865429520607\n",
      "Epoch 393/1000, PINN Loss: 0.14430133998394012\n",
      "Epoch 394/1000, PINN Loss: 0.14069069921970367\n",
      "Epoch 395/1000, PINN Loss: 0.13793514668941498\n",
      "Epoch 396/1000, PINN Loss: 0.1347227841615677\n",
      "Epoch 397/1000, PINN Loss: 0.13049504160881042\n",
      "Epoch 398/1000, PINN Loss: 0.13239194452762604\n",
      "Epoch 399/1000, PINN Loss: 0.1244652196764946\n",
      "Epoch 400/1000, PINN Loss: 0.12151370942592621\n",
      "Epoch 401/1000, PINN Loss: 0.11796609312295914\n",
      "Epoch 402/1000, PINN Loss: 0.11459348350763321\n",
      "Epoch 403/1000, PINN Loss: 0.11261144280433655\n",
      "Epoch 404/1000, PINN Loss: 0.10833054035902023\n",
      "Epoch 405/1000, PINN Loss: 0.10477866977453232\n",
      "Epoch 406/1000, PINN Loss: 0.10342730581760406\n",
      "Epoch 407/1000, PINN Loss: 0.10053504258394241\n",
      "Epoch 408/1000, PINN Loss: 0.09705326706171036\n",
      "Epoch 409/1000, PINN Loss: 0.09609628468751907\n",
      "Epoch 410/1000, PINN Loss: 0.09189905971288681\n",
      "Epoch 411/1000, PINN Loss: 0.08977247774600983\n",
      "Epoch 412/1000, PINN Loss: 0.0867934450507164\n",
      "Epoch 413/1000, PINN Loss: 0.08395189791917801\n",
      "Epoch 414/1000, PINN Loss: 0.08185636252164841\n",
      "Epoch 415/1000, PINN Loss: 0.08011386543512344\n",
      "Epoch 416/1000, PINN Loss: 0.07597445696592331\n",
      "Epoch 417/1000, PINN Loss: 0.07409612834453583\n",
      "Epoch 418/1000, PINN Loss: 0.07183799892663956\n",
      "Epoch 419/1000, PINN Loss: 0.06903693079948425\n",
      "Epoch 420/1000, PINN Loss: 0.06827055662870407\n",
      "Epoch 421/1000, PINN Loss: 0.065129354596138\n",
      "Epoch 422/1000, PINN Loss: 0.06270613521337509\n",
      "Epoch 423/1000, PINN Loss: 0.06016666814684868\n",
      "Epoch 424/1000, PINN Loss: 0.06162465736269951\n",
      "Epoch 425/1000, PINN Loss: 0.056733258068561554\n",
      "Epoch 426/1000, PINN Loss: 0.05448858439922333\n",
      "Epoch 427/1000, PINN Loss: 0.05283437296748161\n",
      "Epoch 428/1000, PINN Loss: 0.05114014819264412\n",
      "Epoch 429/1000, PINN Loss: 0.049262866377830505\n",
      "Epoch 430/1000, PINN Loss: 0.04677934944629669\n",
      "Epoch 431/1000, PINN Loss: 0.04539548605680466\n",
      "Epoch 432/1000, PINN Loss: 0.04395965114235878\n",
      "Epoch 433/1000, PINN Loss: 0.042612720280885696\n",
      "Epoch 434/1000, PINN Loss: 0.040972765535116196\n",
      "Epoch 435/1000, PINN Loss: 0.03903152048587799\n",
      "Epoch 436/1000, PINN Loss: 0.037817519158124924\n",
      "Epoch 437/1000, PINN Loss: 0.03678949922323227\n",
      "Epoch 438/1000, PINN Loss: 0.03493808954954147\n",
      "Epoch 439/1000, PINN Loss: 0.033744361251592636\n",
      "Epoch 440/1000, PINN Loss: 0.03391997888684273\n",
      "Epoch 441/1000, PINN Loss: 0.03132404759526253\n",
      "Epoch 442/1000, PINN Loss: 0.03116530552506447\n",
      "Epoch 443/1000, PINN Loss: 0.02929907850921154\n",
      "Epoch 444/1000, PINN Loss: 0.02857578732073307\n",
      "Epoch 445/1000, PINN Loss: 0.027696698904037476\n",
      "Epoch 446/1000, PINN Loss: 0.026584001258015633\n",
      "Epoch 447/1000, PINN Loss: 0.025870010256767273\n",
      "Epoch 448/1000, PINN Loss: 0.024910397827625275\n",
      "Epoch 449/1000, PINN Loss: 0.02422163262963295\n",
      "Epoch 450/1000, PINN Loss: 0.023719429969787598\n",
      "Epoch 451/1000, PINN Loss: 0.022641632705926895\n",
      "Epoch 452/1000, PINN Loss: 0.022093063220381737\n",
      "Epoch 453/1000, PINN Loss: 0.021257886663079262\n",
      "Epoch 454/1000, PINN Loss: 0.020231198519468307\n",
      "Epoch 455/1000, PINN Loss: 0.020282195881009102\n",
      "Epoch 456/1000, PINN Loss: 0.020328469574451447\n",
      "Epoch 457/1000, PINN Loss: 0.019683778285980225\n",
      "Epoch 458/1000, PINN Loss: 0.019291874021291733\n",
      "Epoch 459/1000, PINN Loss: 0.0180952288210392\n",
      "Epoch 460/1000, PINN Loss: 0.01805560290813446\n",
      "Epoch 461/1000, PINN Loss: 0.01736656203866005\n",
      "Epoch 462/1000, PINN Loss: 0.017397036775946617\n",
      "Epoch 463/1000, PINN Loss: 0.016667170450091362\n",
      "Epoch 464/1000, PINN Loss: 0.01665330119431019\n",
      "Epoch 465/1000, PINN Loss: 0.015661370009183884\n",
      "Epoch 466/1000, PINN Loss: 0.01576712355017662\n",
      "Epoch 467/1000, PINN Loss: 0.015519658103585243\n",
      "Epoch 468/1000, PINN Loss: 0.014929182827472687\n",
      "Epoch 469/1000, PINN Loss: 0.015009528025984764\n",
      "Epoch 470/1000, PINN Loss: 0.014758419245481491\n",
      "Epoch 471/1000, PINN Loss: 0.014988758601248264\n",
      "Epoch 472/1000, PINN Loss: 0.014362314715981483\n",
      "Epoch 473/1000, PINN Loss: 0.014395213685929775\n",
      "Epoch 474/1000, PINN Loss: 0.013974697329103947\n",
      "Epoch 475/1000, PINN Loss: 0.014454633928835392\n",
      "Epoch 476/1000, PINN Loss: 0.013672610744833946\n",
      "Epoch 477/1000, PINN Loss: 0.014480541460216045\n",
      "Epoch 478/1000, PINN Loss: 0.014651176519691944\n",
      "Epoch 479/1000, PINN Loss: 0.013681451790034771\n",
      "Epoch 480/1000, PINN Loss: 0.013549646362662315\n",
      "Epoch 481/1000, PINN Loss: 0.013101496733725071\n",
      "Epoch 482/1000, PINN Loss: 0.013126268982887268\n",
      "Epoch 483/1000, PINN Loss: 0.01337083987891674\n",
      "Epoch 484/1000, PINN Loss: 0.01389206200838089\n",
      "Epoch 485/1000, PINN Loss: 0.013036116026341915\n",
      "Epoch 486/1000, PINN Loss: 0.01292575616389513\n",
      "Epoch 487/1000, PINN Loss: 0.013634227216243744\n",
      "Epoch 488/1000, PINN Loss: 0.01288033090531826\n",
      "Epoch 489/1000, PINN Loss: 0.012751289643347263\n",
      "Epoch 490/1000, PINN Loss: 0.012895792722702026\n",
      "Epoch 491/1000, PINN Loss: 0.012698512524366379\n",
      "Epoch 492/1000, PINN Loss: 0.013705888763070107\n",
      "Epoch 493/1000, PINN Loss: 0.01255874428898096\n",
      "Epoch 494/1000, PINN Loss: 0.013247782364487648\n",
      "Epoch 495/1000, PINN Loss: 0.01421849150210619\n",
      "Epoch 496/1000, PINN Loss: 0.012962603941559792\n",
      "Epoch 497/1000, PINN Loss: 0.012674572877585888\n",
      "Epoch 498/1000, PINN Loss: 0.012646229937672615\n",
      "Epoch 499/1000, PINN Loss: 0.012195896357297897\n",
      "Epoch 500/1000, PINN Loss: 0.012681321240961552\n",
      "Epoch 501/1000, PINN Loss: 0.012473772279918194\n",
      "Epoch 502/1000, PINN Loss: 0.012922514230012894\n",
      "Epoch 503/1000, PINN Loss: 0.012610714882612228\n",
      "Epoch 504/1000, PINN Loss: 0.012565410695970058\n",
      "Epoch 505/1000, PINN Loss: 0.012510379776358604\n",
      "Epoch 506/1000, PINN Loss: 0.01269123237580061\n",
      "Epoch 507/1000, PINN Loss: 0.01249408908188343\n",
      "Epoch 508/1000, PINN Loss: 0.012458110228180885\n",
      "Epoch 509/1000, PINN Loss: 0.01340180542320013\n",
      "Epoch 510/1000, PINN Loss: 0.012845068238675594\n",
      "Epoch 511/1000, PINN Loss: 0.012219233438372612\n",
      "Epoch 512/1000, PINN Loss: 0.012413491494953632\n",
      "Epoch 513/1000, PINN Loss: 0.012469100765883923\n",
      "Epoch 514/1000, PINN Loss: 0.012683087959885597\n",
      "Epoch 515/1000, PINN Loss: 0.012594623491168022\n",
      "Epoch 516/1000, PINN Loss: 0.012638368643820286\n",
      "Epoch 517/1000, PINN Loss: 0.01255764439702034\n",
      "Epoch 518/1000, PINN Loss: 0.012500567361712456\n",
      "Epoch 519/1000, PINN Loss: 0.012502279132604599\n",
      "Epoch 520/1000, PINN Loss: 0.012710255570709705\n",
      "Epoch 521/1000, PINN Loss: 0.012707110494375229\n",
      "Epoch 522/1000, PINN Loss: 0.012440521270036697\n",
      "Epoch 523/1000, PINN Loss: 0.012724753469228745\n",
      "Epoch 524/1000, PINN Loss: 0.01265521440654993\n",
      "Epoch 525/1000, PINN Loss: 0.012502896599471569\n",
      "Epoch 526/1000, PINN Loss: 0.012714575976133347\n",
      "Epoch 527/1000, PINN Loss: 0.012314300052821636\n",
      "Epoch 528/1000, PINN Loss: 0.012723327614367008\n",
      "Epoch 529/1000, PINN Loss: 0.01250221487134695\n",
      "Epoch 530/1000, PINN Loss: 0.012335257604718208\n",
      "Epoch 531/1000, PINN Loss: 0.012440728023648262\n",
      "Epoch 532/1000, PINN Loss: 0.012606077827513218\n",
      "Epoch 533/1000, PINN Loss: 0.012631028890609741\n",
      "Epoch 534/1000, PINN Loss: 0.01231452077627182\n",
      "Epoch 535/1000, PINN Loss: 0.01257253997027874\n",
      "Epoch 536/1000, PINN Loss: 0.012408941052854061\n",
      "Epoch 537/1000, PINN Loss: 0.012400169856846333\n",
      "Epoch 538/1000, PINN Loss: 0.012695588171482086\n",
      "Epoch 539/1000, PINN Loss: 0.012739851139485836\n",
      "Epoch 540/1000, PINN Loss: 0.013209722004830837\n",
      "Epoch 541/1000, PINN Loss: 0.012529113329946995\n",
      "Epoch 542/1000, PINN Loss: 0.012809578329324722\n",
      "Epoch 543/1000, PINN Loss: 0.013198557309806347\n",
      "Epoch 544/1000, PINN Loss: 0.01245134323835373\n",
      "Epoch 545/1000, PINN Loss: 0.012498605996370316\n",
      "Epoch 546/1000, PINN Loss: 0.01306434627622366\n",
      "Epoch 547/1000, PINN Loss: 0.012414160184562206\n",
      "Epoch 548/1000, PINN Loss: 0.015068143606185913\n",
      "Epoch 549/1000, PINN Loss: 0.012237293645739555\n",
      "Epoch 550/1000, PINN Loss: 0.012113120406866074\n",
      "Epoch 551/1000, PINN Loss: 0.012512166053056717\n",
      "Epoch 552/1000, PINN Loss: 0.012627429328858852\n",
      "Epoch 553/1000, PINN Loss: 0.012329173274338245\n",
      "Epoch 554/1000, PINN Loss: 0.012432717718183994\n",
      "Epoch 555/1000, PINN Loss: 0.012397315353155136\n",
      "Epoch 556/1000, PINN Loss: 0.012540250085294247\n",
      "Epoch 557/1000, PINN Loss: 0.012010781094431877\n",
      "Epoch 558/1000, PINN Loss: 0.012525162659585476\n",
      "Epoch 559/1000, PINN Loss: 0.012569383718073368\n",
      "Epoch 560/1000, PINN Loss: 0.012581848539412022\n",
      "Epoch 561/1000, PINN Loss: 0.012276249006390572\n",
      "Epoch 562/1000, PINN Loss: 0.012405378744006157\n",
      "Epoch 563/1000, PINN Loss: 0.012180227786302567\n",
      "Epoch 564/1000, PINN Loss: 0.012615461833775043\n",
      "Epoch 565/1000, PINN Loss: 0.012678793631494045\n",
      "Epoch 566/1000, PINN Loss: 0.012163481675088406\n",
      "Epoch 567/1000, PINN Loss: 0.012806613929569721\n",
      "Epoch 568/1000, PINN Loss: 0.012734622694551945\n",
      "Epoch 569/1000, PINN Loss: 0.012421454302966595\n",
      "Epoch 570/1000, PINN Loss: 0.014804603531956673\n",
      "Epoch 571/1000, PINN Loss: 0.04838906228542328\n",
      "Epoch 572/1000, PINN Loss: 0.017042672261595726\n",
      "Epoch 573/1000, PINN Loss: 0.017207061871886253\n",
      "Epoch 574/1000, PINN Loss: 0.01440114714205265\n",
      "Epoch 575/1000, PINN Loss: 0.013103287667036057\n",
      "Epoch 576/1000, PINN Loss: 0.015755074098706245\n",
      "Epoch 577/1000, PINN Loss: 0.020235756412148476\n",
      "Epoch 578/1000, PINN Loss: 0.013624835759401321\n",
      "Epoch 579/1000, PINN Loss: 0.013766058720648289\n",
      "Epoch 580/1000, PINN Loss: 0.014714349061250687\n",
      "Epoch 581/1000, PINN Loss: 0.015074405819177628\n",
      "Epoch 582/1000, PINN Loss: 0.014029968529939651\n",
      "Epoch 583/1000, PINN Loss: 0.012570508755743504\n",
      "Epoch 584/1000, PINN Loss: 0.015869101509451866\n",
      "Epoch 585/1000, PINN Loss: 0.013029294088482857\n",
      "Epoch 586/1000, PINN Loss: 0.013227944262325764\n",
      "Epoch 587/1000, PINN Loss: 0.012716531753540039\n",
      "Epoch 588/1000, PINN Loss: 0.012533054687082767\n",
      "Epoch 589/1000, PINN Loss: 0.012823405675590038\n",
      "Epoch 590/1000, PINN Loss: 0.012917888350784779\n",
      "Epoch 591/1000, PINN Loss: 0.013039542362093925\n",
      "Epoch 592/1000, PINN Loss: 0.012586696073412895\n",
      "Epoch 593/1000, PINN Loss: 0.012880374677479267\n",
      "Epoch 594/1000, PINN Loss: 0.012736398726701736\n",
      "Epoch 595/1000, PINN Loss: 0.012967955321073532\n",
      "Epoch 596/1000, PINN Loss: 0.012496455572545528\n",
      "Epoch 597/1000, PINN Loss: 0.015117581002414227\n",
      "Epoch 598/1000, PINN Loss: 0.012411877512931824\n",
      "Epoch 599/1000, PINN Loss: 0.012580185197293758\n",
      "Epoch 600/1000, PINN Loss: 0.012825187295675278\n",
      "Epoch 601/1000, PINN Loss: 0.012269867584109306\n",
      "Epoch 602/1000, PINN Loss: 0.012362776324152946\n",
      "Epoch 603/1000, PINN Loss: 0.012158243916928768\n",
      "Epoch 604/1000, PINN Loss: 0.012384762056171894\n",
      "Epoch 605/1000, PINN Loss: 0.012914064340293407\n",
      "Epoch 606/1000, PINN Loss: 0.012909597717225552\n",
      "Epoch 607/1000, PINN Loss: 0.012340724468231201\n",
      "Epoch 608/1000, PINN Loss: 0.012718064710497856\n",
      "Epoch 609/1000, PINN Loss: 0.012544292956590652\n",
      "Epoch 610/1000, PINN Loss: 0.012516766786575317\n",
      "Epoch 611/1000, PINN Loss: 0.012369486503303051\n",
      "Epoch 612/1000, PINN Loss: 0.014250439591705799\n",
      "Epoch 613/1000, PINN Loss: 0.012539183720946312\n",
      "Epoch 614/1000, PINN Loss: 0.012384278699755669\n",
      "Epoch 615/1000, PINN Loss: 0.012433593161404133\n",
      "Epoch 616/1000, PINN Loss: 0.01446952112019062\n",
      "Epoch 617/1000, PINN Loss: 0.012249721214175224\n",
      "Epoch 618/1000, PINN Loss: 0.012209624983370304\n",
      "Epoch 619/1000, PINN Loss: 0.01216778252273798\n",
      "Epoch 620/1000, PINN Loss: 0.011934282258152962\n",
      "Epoch 621/1000, PINN Loss: 0.012391862459480762\n",
      "Epoch 622/1000, PINN Loss: 0.012224570848047733\n",
      "Epoch 623/1000, PINN Loss: 0.01224148366600275\n",
      "Epoch 624/1000, PINN Loss: 0.012158302590250969\n",
      "Epoch 625/1000, PINN Loss: 0.012373027391731739\n",
      "Epoch 626/1000, PINN Loss: 0.012209809385240078\n",
      "Epoch 627/1000, PINN Loss: 0.012634933926165104\n",
      "Epoch 628/1000, PINN Loss: 0.012187374755740166\n",
      "Epoch 629/1000, PINN Loss: 0.01336555927991867\n",
      "Epoch 630/1000, PINN Loss: 0.011879165656864643\n",
      "Epoch 631/1000, PINN Loss: 0.01239505410194397\n",
      "Epoch 632/1000, PINN Loss: 0.012399938888847828\n",
      "Epoch 633/1000, PINN Loss: 0.012469694018363953\n",
      "Epoch 634/1000, PINN Loss: 0.01249140128493309\n",
      "Epoch 635/1000, PINN Loss: 0.01232520304620266\n",
      "Epoch 636/1000, PINN Loss: 0.012684636749327183\n",
      "Epoch 637/1000, PINN Loss: 0.01232446264475584\n",
      "Epoch 638/1000, PINN Loss: 0.01199802290648222\n",
      "Epoch 639/1000, PINN Loss: 0.01227641198784113\n",
      "Epoch 640/1000, PINN Loss: 0.012319843284785748\n",
      "Epoch 641/1000, PINN Loss: 0.011988271959125996\n",
      "Epoch 642/1000, PINN Loss: 0.013702108524739742\n",
      "Epoch 643/1000, PINN Loss: 0.01249662321060896\n",
      "Epoch 644/1000, PINN Loss: 0.012955846264958382\n",
      "Epoch 645/1000, PINN Loss: 0.012346211820840836\n",
      "Epoch 646/1000, PINN Loss: 0.012624412775039673\n",
      "Epoch 647/1000, PINN Loss: 0.01302549708634615\n",
      "Epoch 648/1000, PINN Loss: 0.011952586472034454\n",
      "Epoch 649/1000, PINN Loss: 0.012140022590756416\n",
      "Epoch 650/1000, PINN Loss: 0.01219476480036974\n",
      "Epoch 651/1000, PINN Loss: 0.012145811691880226\n",
      "Epoch 652/1000, PINN Loss: 0.01266702264547348\n",
      "Epoch 653/1000, PINN Loss: 0.012225775048136711\n",
      "Epoch 654/1000, PINN Loss: 0.011750242672860622\n",
      "Epoch 655/1000, PINN Loss: 0.012069737538695335\n",
      "Epoch 656/1000, PINN Loss: 0.012387137860059738\n",
      "Epoch 657/1000, PINN Loss: 0.012093774043023586\n",
      "Epoch 658/1000, PINN Loss: 0.012170222587883472\n",
      "Epoch 659/1000, PINN Loss: 0.011847240850329399\n",
      "Epoch 660/1000, PINN Loss: 0.015237188898026943\n",
      "Epoch 661/1000, PINN Loss: 0.013224928639829159\n",
      "Epoch 662/1000, PINN Loss: 0.012115435674786568\n",
      "Epoch 663/1000, PINN Loss: 0.012587559409439564\n",
      "Epoch 664/1000, PINN Loss: 0.012121060863137245\n",
      "Epoch 665/1000, PINN Loss: 0.013174513354897499\n",
      "Epoch 666/1000, PINN Loss: 0.011891153641045094\n",
      "Epoch 667/1000, PINN Loss: 0.012239701114594936\n",
      "Epoch 668/1000, PINN Loss: 0.012537999078631401\n",
      "Epoch 669/1000, PINN Loss: 0.012969356030225754\n",
      "Epoch 670/1000, PINN Loss: 0.012260696850717068\n",
      "Epoch 671/1000, PINN Loss: 0.012207199819386005\n",
      "Epoch 672/1000, PINN Loss: 0.012020750902593136\n",
      "Epoch 673/1000, PINN Loss: 0.011949868872761726\n",
      "Epoch 674/1000, PINN Loss: 0.01261978130787611\n",
      "Epoch 675/1000, PINN Loss: 0.012125610373914242\n",
      "Epoch 676/1000, PINN Loss: 0.011946612037718296\n",
      "Epoch 677/1000, PINN Loss: 0.017123842611908913\n",
      "Epoch 678/1000, PINN Loss: 0.014047752134501934\n",
      "Epoch 679/1000, PINN Loss: 0.012189776636660099\n",
      "Epoch 680/1000, PINN Loss: 0.012898915447294712\n",
      "Epoch 681/1000, PINN Loss: 0.012253869324922562\n",
      "Epoch 682/1000, PINN Loss: 0.012023692019283772\n",
      "Epoch 683/1000, PINN Loss: 0.011921843513846397\n",
      "Epoch 684/1000, PINN Loss: 0.018436461687088013\n",
      "Epoch 685/1000, PINN Loss: 0.012038469314575195\n",
      "Epoch 686/1000, PINN Loss: 0.012106862850487232\n",
      "Epoch 687/1000, PINN Loss: 0.012116452679038048\n",
      "Epoch 688/1000, PINN Loss: 0.011912237852811813\n",
      "Epoch 689/1000, PINN Loss: 0.012155747041106224\n",
      "Epoch 690/1000, PINN Loss: 0.012932514771819115\n",
      "Epoch 691/1000, PINN Loss: 0.012095904909074306\n",
      "Epoch 692/1000, PINN Loss: 0.011937913484871387\n",
      "Epoch 693/1000, PINN Loss: 0.012156619690358639\n",
      "Epoch 694/1000, PINN Loss: 0.011818943545222282\n",
      "Epoch 695/1000, PINN Loss: 0.012097574770450592\n",
      "Epoch 696/1000, PINN Loss: 0.011926544830203056\n",
      "Epoch 697/1000, PINN Loss: 0.011764930561184883\n",
      "Epoch 698/1000, PINN Loss: 0.013107303529977798\n",
      "Epoch 699/1000, PINN Loss: 0.012009713798761368\n",
      "Epoch 700/1000, PINN Loss: 0.012142620980739594\n",
      "Epoch 701/1000, PINN Loss: 0.012377794831991196\n",
      "Epoch 702/1000, PINN Loss: 0.011917171068489552\n",
      "Epoch 703/1000, PINN Loss: 0.012134794145822525\n",
      "Epoch 704/1000, PINN Loss: 0.012045438401401043\n",
      "Epoch 705/1000, PINN Loss: 0.011904383078217506\n",
      "Epoch 706/1000, PINN Loss: 0.012076372280716896\n",
      "Epoch 707/1000, PINN Loss: 0.013961484655737877\n",
      "Epoch 708/1000, PINN Loss: 0.012845424935221672\n",
      "Epoch 709/1000, PINN Loss: 0.012119486927986145\n",
      "Epoch 710/1000, PINN Loss: 0.011906733736395836\n",
      "Epoch 711/1000, PINN Loss: 0.012168154120445251\n",
      "Epoch 712/1000, PINN Loss: 0.012054420076310635\n",
      "Epoch 713/1000, PINN Loss: 0.011631843633949757\n",
      "Epoch 714/1000, PINN Loss: 0.01205375511199236\n",
      "Epoch 715/1000, PINN Loss: 0.011827592737972736\n",
      "Epoch 716/1000, PINN Loss: 0.012173052877187729\n",
      "Epoch 717/1000, PINN Loss: 0.011929228901863098\n",
      "Epoch 718/1000, PINN Loss: 0.012079167179763317\n",
      "Epoch 719/1000, PINN Loss: 0.01193268783390522\n",
      "Epoch 720/1000, PINN Loss: 0.011975797824561596\n",
      "Epoch 721/1000, PINN Loss: 0.011962417513132095\n",
      "Epoch 722/1000, PINN Loss: 0.012043043971061707\n",
      "Epoch 723/1000, PINN Loss: 0.012075334787368774\n",
      "Epoch 724/1000, PINN Loss: 0.011894811876118183\n",
      "Epoch 725/1000, PINN Loss: 0.012458892539143562\n",
      "Epoch 726/1000, PINN Loss: 0.013908776454627514\n",
      "Epoch 727/1000, PINN Loss: 0.012112091295421124\n",
      "Epoch 728/1000, PINN Loss: 0.011763092130422592\n",
      "Epoch 729/1000, PINN Loss: 0.012060318142175674\n",
      "Epoch 730/1000, PINN Loss: 0.012087292037904263\n",
      "Epoch 731/1000, PINN Loss: 0.012542751617729664\n",
      "Epoch 732/1000, PINN Loss: 0.011831261217594147\n",
      "Epoch 733/1000, PINN Loss: 0.011963430792093277\n",
      "Epoch 734/1000, PINN Loss: 0.011989600956439972\n",
      "Epoch 735/1000, PINN Loss: 0.011947047896683216\n",
      "Epoch 736/1000, PINN Loss: 0.01180200930684805\n",
      "Epoch 737/1000, PINN Loss: 0.012059919536113739\n",
      "Epoch 738/1000, PINN Loss: 0.012081959284842014\n",
      "Epoch 739/1000, PINN Loss: 0.011998225934803486\n",
      "Epoch 740/1000, PINN Loss: 0.011856586672365665\n",
      "Epoch 741/1000, PINN Loss: 0.01195335853844881\n",
      "Epoch 742/1000, PINN Loss: 0.012385601177811623\n",
      "Epoch 743/1000, PINN Loss: 0.012307636439800262\n",
      "Epoch 744/1000, PINN Loss: 0.011929798871278763\n",
      "Epoch 745/1000, PINN Loss: 0.0118344034999609\n",
      "Epoch 746/1000, PINN Loss: 0.012134183198213577\n",
      "Epoch 747/1000, PINN Loss: 0.011831533163785934\n",
      "Epoch 748/1000, PINN Loss: 0.011666317470371723\n",
      "Epoch 749/1000, PINN Loss: 0.011852037161588669\n",
      "Epoch 750/1000, PINN Loss: 0.01173250749707222\n",
      "Epoch 751/1000, PINN Loss: 0.011983418837189674\n",
      "Epoch 752/1000, PINN Loss: 0.011620759963989258\n",
      "Epoch 753/1000, PINN Loss: 0.011931147426366806\n",
      "Epoch 754/1000, PINN Loss: 0.01654013618826866\n",
      "Epoch 755/1000, PINN Loss: 0.012066707946360111\n",
      "Epoch 756/1000, PINN Loss: 0.012580585666000843\n",
      "Epoch 757/1000, PINN Loss: 0.012181212194263935\n",
      "Epoch 758/1000, PINN Loss: 0.011938346549868584\n",
      "Epoch 759/1000, PINN Loss: 0.011810414493083954\n",
      "Epoch 760/1000, PINN Loss: 0.012025418691337109\n",
      "Epoch 761/1000, PINN Loss: 0.011852797120809555\n",
      "Epoch 762/1000, PINN Loss: 0.012115271762013435\n",
      "Epoch 763/1000, PINN Loss: 0.01202383078634739\n",
      "Epoch 764/1000, PINN Loss: 0.01186657976359129\n",
      "Epoch 765/1000, PINN Loss: 0.012119666673243046\n",
      "Epoch 766/1000, PINN Loss: 0.012490671128034592\n",
      "Epoch 767/1000, PINN Loss: 0.011773196049034595\n",
      "Epoch 768/1000, PINN Loss: 0.011697483249008656\n",
      "Epoch 769/1000, PINN Loss: 0.011538829654455185\n",
      "Epoch 770/1000, PINN Loss: 0.011913733556866646\n",
      "Epoch 771/1000, PINN Loss: 0.011660278774797916\n",
      "Epoch 772/1000, PINN Loss: 0.01162369828671217\n",
      "Epoch 773/1000, PINN Loss: 0.011928072199225426\n",
      "Epoch 774/1000, PINN Loss: 0.012018335051834583\n",
      "Epoch 775/1000, PINN Loss: 0.01177308987826109\n",
      "Epoch 776/1000, PINN Loss: 0.011682143434882164\n",
      "Epoch 777/1000, PINN Loss: 0.011889553628861904\n",
      "Epoch 778/1000, PINN Loss: 0.01339536253362894\n",
      "Epoch 779/1000, PINN Loss: 0.012434925884008408\n",
      "Epoch 780/1000, PINN Loss: 0.01198050007224083\n",
      "Epoch 781/1000, PINN Loss: 0.013993198052048683\n",
      "Epoch 782/1000, PINN Loss: 0.0115357032045722\n",
      "Epoch 783/1000, PINN Loss: 0.01212410070002079\n",
      "Epoch 784/1000, PINN Loss: 0.011718652211129665\n",
      "Epoch 785/1000, PINN Loss: 0.011753234080970287\n",
      "Epoch 786/1000, PINN Loss: 0.011832098476588726\n",
      "Epoch 787/1000, PINN Loss: 0.01166814099997282\n",
      "Epoch 788/1000, PINN Loss: 0.011282023042440414\n",
      "Epoch 789/1000, PINN Loss: 0.011745383962988853\n",
      "Epoch 790/1000, PINN Loss: 0.011854924261569977\n",
      "Epoch 791/1000, PINN Loss: 0.011835138313472271\n",
      "Epoch 792/1000, PINN Loss: 0.01170392520725727\n",
      "Epoch 793/1000, PINN Loss: 0.011644884943962097\n",
      "Epoch 794/1000, PINN Loss: 0.012247550301253796\n",
      "Epoch 795/1000, PINN Loss: 0.012074356898665428\n",
      "Epoch 796/1000, PINN Loss: 0.011803879402577877\n",
      "Epoch 797/1000, PINN Loss: 0.011829118244349957\n",
      "Epoch 798/1000, PINN Loss: 0.011513851583003998\n",
      "Epoch 799/1000, PINN Loss: 0.011769731529057026\n",
      "Epoch 800/1000, PINN Loss: 0.012067795731127262\n",
      "Epoch 801/1000, PINN Loss: 0.01169926393777132\n",
      "Epoch 802/1000, PINN Loss: 0.01137332059442997\n",
      "Epoch 803/1000, PINN Loss: 0.014855733141303062\n",
      "Epoch 804/1000, PINN Loss: 0.012182610109448433\n",
      "Epoch 805/1000, PINN Loss: 0.011615636758506298\n",
      "Epoch 806/1000, PINN Loss: 0.01203080639243126\n",
      "Epoch 807/1000, PINN Loss: 0.013059890829026699\n",
      "Epoch 808/1000, PINN Loss: 0.012489542365074158\n",
      "Epoch 809/1000, PINN Loss: 0.01128319650888443\n",
      "Epoch 810/1000, PINN Loss: 0.011728719808161259\n",
      "Epoch 811/1000, PINN Loss: 0.011891036294400692\n",
      "Epoch 812/1000, PINN Loss: 0.011744482442736626\n",
      "Epoch 813/1000, PINN Loss: 0.012552790343761444\n",
      "Epoch 814/1000, PINN Loss: 0.03142969682812691\n",
      "Epoch 815/1000, PINN Loss: 0.012540585361421108\n",
      "Epoch 816/1000, PINN Loss: 0.012923585250973701\n",
      "Epoch 817/1000, PINN Loss: 0.01284006331115961\n",
      "Epoch 818/1000, PINN Loss: 0.01231474056839943\n",
      "Epoch 819/1000, PINN Loss: 0.012250724248588085\n",
      "Epoch 820/1000, PINN Loss: 0.012604217045009136\n",
      "Epoch 821/1000, PINN Loss: 0.012949354946613312\n",
      "Epoch 822/1000, PINN Loss: 0.012325883843004704\n",
      "Epoch 823/1000, PINN Loss: 0.012470298446714878\n",
      "Epoch 824/1000, PINN Loss: 0.011715789325535297\n",
      "Epoch 825/1000, PINN Loss: 0.011911600828170776\n",
      "Epoch 826/1000, PINN Loss: 0.012272273190319538\n",
      "Epoch 827/1000, PINN Loss: 0.011925729922950268\n",
      "Epoch 828/1000, PINN Loss: 0.011680238880217075\n",
      "Epoch 829/1000, PINN Loss: 0.012134324759244919\n",
      "Epoch 830/1000, PINN Loss: 0.012093544006347656\n",
      "Epoch 831/1000, PINN Loss: 0.011801336891949177\n",
      "Epoch 832/1000, PINN Loss: 0.01270991936326027\n",
      "Epoch 833/1000, PINN Loss: 0.012856385670602322\n",
      "Epoch 834/1000, PINN Loss: 0.011960788629949093\n",
      "Epoch 835/1000, PINN Loss: 0.013131835497915745\n",
      "Epoch 836/1000, PINN Loss: 0.011914841830730438\n",
      "Epoch 837/1000, PINN Loss: 0.011753524653613567\n",
      "Epoch 838/1000, PINN Loss: 0.013788821175694466\n",
      "Epoch 839/1000, PINN Loss: 0.012564071454107761\n",
      "Epoch 840/1000, PINN Loss: 0.011388218961656094\n",
      "Epoch 841/1000, PINN Loss: 0.01186845637857914\n",
      "Epoch 842/1000, PINN Loss: 0.011512183584272861\n",
      "Epoch 843/1000, PINN Loss: 0.012058630585670471\n",
      "Epoch 844/1000, PINN Loss: 0.01158962957561016\n",
      "Epoch 845/1000, PINN Loss: 0.011786864139139652\n",
      "Epoch 846/1000, PINN Loss: 0.012212641537189484\n",
      "Epoch 847/1000, PINN Loss: 0.011628828942775726\n",
      "Epoch 848/1000, PINN Loss: 0.011852634139358997\n",
      "Epoch 849/1000, PINN Loss: 0.012130995281040668\n",
      "Epoch 850/1000, PINN Loss: 0.011553185991942883\n",
      "Epoch 851/1000, PINN Loss: 0.011432146653532982\n",
      "Epoch 852/1000, PINN Loss: 0.011485990136861801\n",
      "Epoch 853/1000, PINN Loss: 0.011591589078307152\n",
      "Epoch 854/1000, PINN Loss: 0.011700635775923729\n",
      "Epoch 855/1000, PINN Loss: 0.011733966879546642\n",
      "Epoch 856/1000, PINN Loss: 0.011539123021066189\n",
      "Epoch 857/1000, PINN Loss: 0.01133032701909542\n",
      "Epoch 858/1000, PINN Loss: 0.01148151233792305\n",
      "Epoch 859/1000, PINN Loss: 0.012065649032592773\n",
      "Epoch 860/1000, PINN Loss: 0.011321553960442543\n",
      "Epoch 861/1000, PINN Loss: 0.011738992296159267\n",
      "Epoch 862/1000, PINN Loss: 0.011392680928111076\n",
      "Epoch 863/1000, PINN Loss: 0.011020085774362087\n",
      "Epoch 864/1000, PINN Loss: 0.0113559914752841\n",
      "Epoch 865/1000, PINN Loss: 0.012101231142878532\n",
      "Epoch 866/1000, PINN Loss: 0.011642005294561386\n",
      "Epoch 867/1000, PINN Loss: 0.011378781870007515\n",
      "Epoch 868/1000, PINN Loss: 0.011705298908054829\n",
      "Epoch 869/1000, PINN Loss: 0.011524862609803677\n",
      "Epoch 870/1000, PINN Loss: 0.01156898308545351\n",
      "Epoch 871/1000, PINN Loss: 0.011500239372253418\n",
      "Epoch 872/1000, PINN Loss: 0.011612768284976482\n",
      "Epoch 873/1000, PINN Loss: 0.01159367710351944\n",
      "Epoch 874/1000, PINN Loss: 0.011694470420479774\n",
      "Epoch 875/1000, PINN Loss: 0.012058219872415066\n",
      "Epoch 876/1000, PINN Loss: 0.011576876975595951\n",
      "Epoch 877/1000, PINN Loss: 0.011639813892543316\n",
      "Epoch 878/1000, PINN Loss: 0.012509751133620739\n",
      "Epoch 879/1000, PINN Loss: 0.01150415651500225\n",
      "Epoch 880/1000, PINN Loss: 0.012424594722688198\n",
      "Epoch 881/1000, PINN Loss: 0.011523416265845299\n",
      "Epoch 882/1000, PINN Loss: 0.01142172235995531\n",
      "Epoch 883/1000, PINN Loss: 0.011816754005849361\n",
      "Epoch 884/1000, PINN Loss: 0.012756779789924622\n",
      "Epoch 885/1000, PINN Loss: 0.01231679879128933\n",
      "Epoch 886/1000, PINN Loss: 0.011659708805382252\n",
      "Epoch 887/1000, PINN Loss: 0.012041710317134857\n",
      "Epoch 888/1000, PINN Loss: 0.011508519761264324\n",
      "Epoch 889/1000, PINN Loss: 0.01175274234265089\n",
      "Epoch 890/1000, PINN Loss: 0.012280618771910667\n",
      "Epoch 891/1000, PINN Loss: 0.011368864215910435\n",
      "Epoch 892/1000, PINN Loss: 0.014388678595423698\n",
      "Epoch 893/1000, PINN Loss: 0.013004421256482601\n",
      "Epoch 894/1000, PINN Loss: 0.011502278037369251\n",
      "Epoch 895/1000, PINN Loss: 0.011267498135566711\n",
      "Epoch 896/1000, PINN Loss: 0.011651071719825268\n",
      "Epoch 897/1000, PINN Loss: 0.011542916297912598\n",
      "Epoch 898/1000, PINN Loss: 0.011126971803605556\n",
      "Epoch 899/1000, PINN Loss: 0.011480306275188923\n",
      "Epoch 900/1000, PINN Loss: 0.01152723841369152\n",
      "Epoch 901/1000, PINN Loss: 0.011004609055817127\n",
      "Epoch 902/1000, PINN Loss: 0.01160739827901125\n",
      "Epoch 903/1000, PINN Loss: 0.011312425136566162\n",
      "Epoch 904/1000, PINN Loss: 0.011357804760336876\n",
      "Epoch 905/1000, PINN Loss: 0.01128897164016962\n",
      "Epoch 906/1000, PINN Loss: 0.011187000200152397\n",
      "Epoch 907/1000, PINN Loss: 0.011224321089684963\n",
      "Epoch 908/1000, PINN Loss: 0.011475495994091034\n",
      "Epoch 909/1000, PINN Loss: 0.011419286951422691\n",
      "Epoch 910/1000, PINN Loss: 0.012453993782401085\n",
      "Epoch 911/1000, PINN Loss: 0.011250684969127178\n",
      "Epoch 912/1000, PINN Loss: 0.011924461461603642\n",
      "Epoch 913/1000, PINN Loss: 0.01153578981757164\n",
      "Epoch 914/1000, PINN Loss: 0.011473611928522587\n",
      "Epoch 915/1000, PINN Loss: 0.011155911721289158\n",
      "Epoch 916/1000, PINN Loss: 0.011460061185061932\n",
      "Epoch 917/1000, PINN Loss: 0.01126309484243393\n",
      "Epoch 918/1000, PINN Loss: 0.011617884039878845\n",
      "Epoch 919/1000, PINN Loss: 0.011755277402698994\n",
      "Epoch 920/1000, PINN Loss: 0.011371773667633533\n",
      "Epoch 921/1000, PINN Loss: 0.010888054035604\n",
      "Epoch 922/1000, PINN Loss: 0.011408055201172829\n",
      "Epoch 923/1000, PINN Loss: 0.011321065947413445\n",
      "Epoch 924/1000, PINN Loss: 0.011348996311426163\n",
      "Epoch 925/1000, PINN Loss: 0.011250307783484459\n",
      "Epoch 926/1000, PINN Loss: 0.011299938894808292\n",
      "Epoch 927/1000, PINN Loss: 0.013337068259716034\n",
      "Epoch 928/1000, PINN Loss: 0.011196060106158257\n",
      "Epoch 929/1000, PINN Loss: 0.015561909414827824\n",
      "Epoch 930/1000, PINN Loss: 0.011163687333464622\n",
      "Epoch 931/1000, PINN Loss: 0.011650433763861656\n",
      "Epoch 932/1000, PINN Loss: 0.01116526685655117\n",
      "Epoch 933/1000, PINN Loss: 0.011135742999613285\n",
      "Epoch 934/1000, PINN Loss: 0.010967136360704899\n",
      "Epoch 935/1000, PINN Loss: 0.011244404129683971\n",
      "Epoch 936/1000, PINN Loss: 0.012376180849969387\n",
      "Epoch 937/1000, PINN Loss: 0.011334427632391453\n",
      "Epoch 938/1000, PINN Loss: 0.011214765720069408\n",
      "Epoch 939/1000, PINN Loss: 0.011405875906348228\n",
      "Epoch 940/1000, PINN Loss: 0.01092891301959753\n",
      "Epoch 941/1000, PINN Loss: 0.010815504007041454\n",
      "Epoch 942/1000, PINN Loss: 0.01120677962899208\n",
      "Epoch 943/1000, PINN Loss: 0.011188874021172523\n",
      "Epoch 944/1000, PINN Loss: 0.011870013549923897\n",
      "Epoch 945/1000, PINN Loss: 0.011340253986418247\n",
      "Epoch 946/1000, PINN Loss: 0.011437620967626572\n",
      "Epoch 947/1000, PINN Loss: 0.011119114235043526\n",
      "Epoch 948/1000, PINN Loss: 0.011253977194428444\n",
      "Epoch 949/1000, PINN Loss: 0.011432415805757046\n",
      "Epoch 950/1000, PINN Loss: 0.011197417974472046\n",
      "Epoch 951/1000, PINN Loss: 0.010920784436166286\n",
      "Epoch 952/1000, PINN Loss: 0.011708371341228485\n",
      "Epoch 953/1000, PINN Loss: 0.011052321642637253\n",
      "Epoch 954/1000, PINN Loss: 0.010894913226366043\n",
      "Epoch 955/1000, PINN Loss: 0.011115695349872112\n",
      "Epoch 956/1000, PINN Loss: 0.011503344401717186\n",
      "Epoch 957/1000, PINN Loss: 0.011832162737846375\n",
      "Epoch 958/1000, PINN Loss: 0.011178333312273026\n",
      "Epoch 959/1000, PINN Loss: 0.011688166297972202\n",
      "Epoch 960/1000, PINN Loss: 0.011444585397839546\n",
      "Epoch 961/1000, PINN Loss: 0.011437044478952885\n",
      "Epoch 962/1000, PINN Loss: 0.011102387681603432\n",
      "Epoch 963/1000, PINN Loss: 0.010956758633255959\n",
      "Epoch 964/1000, PINN Loss: 0.011308158747851849\n",
      "Epoch 965/1000, PINN Loss: 0.011629861779510975\n",
      "Epoch 966/1000, PINN Loss: 0.012993264012038708\n",
      "Epoch 967/1000, PINN Loss: 0.010841571725904942\n",
      "Epoch 968/1000, PINN Loss: 0.011091201566159725\n",
      "Epoch 969/1000, PINN Loss: 0.011574442498385906\n",
      "Epoch 970/1000, PINN Loss: 0.011008745059370995\n",
      "Epoch 971/1000, PINN Loss: 0.011059543117880821\n",
      "Epoch 972/1000, PINN Loss: 0.011044141836464405\n",
      "Epoch 973/1000, PINN Loss: 0.012536692433059216\n",
      "Epoch 974/1000, PINN Loss: 0.011113648302853107\n",
      "Epoch 975/1000, PINN Loss: 0.011470985598862171\n",
      "Epoch 976/1000, PINN Loss: 0.010918513871729374\n",
      "Epoch 977/1000, PINN Loss: 0.010771061293780804\n",
      "Epoch 978/1000, PINN Loss: 0.011009600013494492\n",
      "Epoch 979/1000, PINN Loss: 0.011055966839194298\n",
      "Epoch 980/1000, PINN Loss: 0.011048514395952225\n",
      "Epoch 981/1000, PINN Loss: 0.010984625667333603\n",
      "Epoch 982/1000, PINN Loss: 0.011303429491817951\n",
      "Epoch 983/1000, PINN Loss: 0.010976494289934635\n",
      "Epoch 984/1000, PINN Loss: 0.011045172810554504\n",
      "Epoch 985/1000, PINN Loss: 0.011166681535542011\n",
      "Epoch 986/1000, PINN Loss: 0.011023728176951408\n",
      "Epoch 987/1000, PINN Loss: 0.011212529614567757\n",
      "Epoch 988/1000, PINN Loss: 0.0110169043764472\n",
      "Epoch 989/1000, PINN Loss: 0.011043647304177284\n",
      "Epoch 990/1000, PINN Loss: 0.011015229858458042\n",
      "Epoch 991/1000, PINN Loss: 0.011165768839418888\n",
      "Epoch 992/1000, PINN Loss: 0.011100754141807556\n",
      "Epoch 993/1000, PINN Loss: 0.028443261981010437\n",
      "Epoch 994/1000, PINN Loss: 0.011760110035538673\n",
      "Epoch 995/1000, PINN Loss: 0.012728031724691391\n",
      "Epoch 996/1000, PINN Loss: 0.011940421536564827\n",
      "Epoch 997/1000, PINN Loss: 0.011299405246973038\n",
      "Epoch 998/1000, PINN Loss: 0.011305553838610649\n",
      "Epoch 999/1000, PINN Loss: 0.011886735446751118\n",
      "GIF saved as pinn_solution_evolution.gif\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "# Define the PINN model with Dropout for ODE u'(t) = cos(2*pi*t)\n",
    "def create_pinn_model(dropout_rate, num_neurons):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(num_neurons, activation='tanh', input_shape=(1,)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(num_neurons, activation='tanh'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(1, activation=None)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Physics-informed loss function\n",
    "def physics_loss(model, t, omega1=1, omega2=1, omega3=0.001, omega4=0.001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(t)\n",
    "        u_pred = model(t, training=True)\n",
    "    u_t = tape.gradient(u_pred, t)\n",
    "    \n",
    "    # 1. Loss t phng trnh vi phn (PDE loss)\n",
    "    loss_eq = tf.reduce_mean((u_t - tf.cos(2 * np.pi * t))**2)\n",
    "    \n",
    "    # 2. Loss t iu kin ban u (Initial Condition loss)\n",
    "    loss_ic = tf.reduce_mean(model(tf.zeros((1, 1)), training=True)**2)\n",
    "\n",
    "    # 3. Loss t d liu thc nghim (Data loss)\n",
    "    u_exact = (1 / (2 * np.pi)) * tf.sin(2 * np.pi * t)\n",
    "    loss_data = tf.reduce_mean((u_pred - u_exact)**2)\n",
    "\n",
    "    # 4. Loss t iu kin bin (Boundary Condition loss) [C th chnh sa]\n",
    "    t_bc = tf.constant([[1.0]])  # iu kin bin ti t = 1\n",
    "    u_bc_pred = model(t_bc, training=True)\n",
    "    u_bc_exact = tf.constant([[0.0]])  # Gi tr ng ti bin (c th cn chnh)\n",
    "    loss_bc = tf.reduce_mean((u_bc_pred - u_bc_exact)**2)\n",
    "\n",
    "    total_loss = omega1 * loss_eq + omega2 * loss_ic + omega3 * loss_data + omega4 * loss_bc\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Training points\n",
    "t_train = tf.reshape(tf.linspace(0.0, 1.0, 100), (-1, 1))\n",
    "\n",
    "# Objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    num_neurons = trial.suggest_categorical('num_neurons', [8, 16, 32, 64, 128])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = create_pinn_model(dropout_rate, num_neurons)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    for epoch in range(2000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_pinn = physics_loss(model, t_train)\n",
    "        gradients = tape.gradient(loss_pinn, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    final_loss = physics_loss(model, t_train).numpy()\n",
    "    return final_loss\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print('Best parameters:', study.best_params)\n",
    "\n",
    "# Train final model with optimal parameters\n",
    "best_params = study.best_params\n",
    "model = create_pinn_model(best_params['dropout_rate'], best_params['num_neurons'])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "img_dir = 'training_frames'\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "frames = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_pinn = physics_loss(model, t_train)\n",
    "    gradients = tape.gradient(loss_pinn, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}/{num_epochs}, PINN Loss: {loss_pinn.numpy()}')\n",
    "\n",
    "        t_test = tf.reshape(tf.linspace(0, 1, 100), (-1, 1))\n",
    "        u_pred = model(t_test)\n",
    "        u_exact = (1 / (2 * np.pi)) * np.sin(2 * np.pi * t_test.numpy())\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(t_test.numpy(), u_pred.numpy(), label=f'PINN Epoch {epoch}')\n",
    "        plt.plot(t_test.numpy(), u_exact, '--k', label='Exact Solution')\n",
    "        plt.legend()\n",
    "        plt.xlabel('t')\n",
    "        plt.ylabel('u(t)')\n",
    "        plt.title(f'PINN ODE Solution at Epoch {epoch}')\n",
    "        plt.ylim(-0.3, 0.3)\n",
    "\n",
    "        frame_path = f'{img_dir}/frame_{epoch}.png'\n",
    "        plt.savefig(frame_path)\n",
    "        frames.append(imageio.imread(frame_path))\n",
    "        plt.close()\n",
    "\n",
    "# Create and save GIF\n",
    "imageio.mimsave('pinn_solution_evolution.gif', frames, duration=5)\n",
    "print('GIF saved as pinn_solution_evolution.gif')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
